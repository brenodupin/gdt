{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8abff2ef",
   "metadata": {},
   "source": [
    "### Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bdf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "\n",
    "import gdt\n",
    "\n",
    "RE_ID = re.compile(r\"ID=([^;]+)\")\n",
    "RE_gene = re.compile(r\"gene=([^;]+)\")\n",
    "RE_dbxref_GeneID = re.compile(r\"Dbxref=.*GeneID:([^;,]+)\")\n",
    "\n",
    "\n",
    "def increment_gdict_file(path):\n",
    "    \"\"\"Increment the GDICT file name by 1.\n",
    "\n",
    "    Example: fungi-ncbi_pilot_03.gdict -> fungi-ncbi_pilot_04.gdict\n",
    "    \"\"\"\n",
    "    plist = path.stem.split(\"_\")\n",
    "    if plist[-1] == \"stripped\":\n",
    "        plist[-1] = \"pilot\"\n",
    "        plist.append(0)\n",
    "\n",
    "    try:\n",
    "        number = int(plist[-1]) + 1\n",
    "        plist[-1] = f\"{number:02d}\"\n",
    "    except ValueError:\n",
    "        raise ValueError(\n",
    "            f\"Invalid GDICT file name: {path.name}. Expected format: <preferred_name>_##.gdict, where ## is a number.\"\n",
    "        )\n",
    "    return path.parent / f'{\"_\".join(plist)}{path.suffix}', number\n",
    "\n",
    "\n",
    "def most_recent_gdict(dir_path, prefix):\n",
    "    \"\"\"Get the most recent gdict file in the directory.\n",
    "\n",
    "    Arguments:\n",
    "        dir_path (Path): Directory to search for GDICT files.\n",
    "        prefix (str): Prefix of the GDICT files to search for. It will match files like \"*<prefix>*.gdict\".\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the most recent GDICT file.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_files = list(\n",
    "        dir_path.glob(f\"*{prefix}*.gdict\")\n",
    "    )  # maybe change to check for numbers after prefix?\n",
    "    if not temp_files:\n",
    "        return dir_path / f\"{prefix}_00.gdict\"\n",
    "    return gdt.natural_sort(temp_files, key=lambda x: x.stem)[-1]\n",
    "\n",
    "\n",
    "def data_process(\n",
    "    df_missing,\n",
    "    an,\n",
    "    gene_dict,\n",
    "    temp_gene_dict,\n",
    "    gct,\n",
    "    temp_count,\n",
    "    log,\n",
    "    use_col=\"desc\",\n",
    "    temp_name=\"temp_desc\",\n",
    "    c_text=\"ncbi_desc\",\n",
    "    gn_tag=\"NCBI\",\n",
    "):\n",
    "    \"\"\"Process the data in the dataframe and update the gene_dict and corresponding temp_gene_dict.\n",
    "\n",
    "    Args:\n",
    "        df_missing (pd.DataFrame): DataFrame containing missing gene information.\n",
    "        an (str): Annotation source identifier.\n",
    "        gene_dict (GeneDict): Dictionary to store gene information.\n",
    "        temp_gene_dict (GeneDict): Temporary dictionary for gene descriptions.\n",
    "        gct (str): Genetic compartment, MIT or PLT (for now).\n",
    "        temp_count (int): Counter for temporary labels.\n",
    "        log (GDTLogger): Logger instance for logging debug information.\n",
    "        use_col (str): Column to use for checking against gene_dict, defaults to 'desc'.\n",
    "        temp_name (str): Name for the temporary dictionary.\n",
    "        c_text (str): Text to be used in the '#c' field of the GeneDbxref object.\n",
    "        gn_tag (str): Tag for the source of the gene description.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated gene_dict, temp_gene_dict, and temp_count.\n",
    "\n",
    "    \"\"\"\n",
    "    for row in df_missing.itertuples():\n",
    "        check_var = getattr(row, use_col)\n",
    "        extra = f\" | ncbi_desc: {row.desc}\" if use_col != \"desc\" else \"\"\n",
    "\n",
    "        log.trace(f\"gene_id: {row.gene_id} | dbxref: {row.dbxref} | att: {row.attributes}\")\n",
    "        log.trace(f\"\\t{check_var = } | {use_col = }\")\n",
    "        log.trace(\n",
    "            f\"\\t {row.desc = } | {row.gene = } | {row.gene_symbol = } | {row.other_aliases = }\"\n",
    "        )\n",
    "\n",
    "        if check_var in gene_dict:\n",
    "            gene_label = gene_dict[check_var].label\n",
    "            log.trace(\n",
    "                f\"\\t[1st T]check_var found in gene_dict, L: |{gene_label}| adding: {row.gene_id} #dx {an}:{row.dbxref} #c {c_text}: {check_var}{extra}\"\n",
    "            )\n",
    "            gene_dict[row.gene_id] = gdt.DbxrefGeneID(\n",
    "                label=gene_label,\n",
    "                an_source=an,\n",
    "                GeneID=row.dbxref,\n",
    "                c=f\"{c_text}: {check_var}{extra}\",\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            log.trace(\n",
    "                f\"\\t[1st F]check_var not found in gene_dict | checking {temp_name} | Label: {check_var}\"\n",
    "            )\n",
    "\n",
    "            if check_var in temp_gene_dict:\n",
    "                gene_label = temp_gene_dict[check_var].label\n",
    "                log.trace(\n",
    "                    f\"\\t[2nd T]check_var found in {temp_name}, L: |{gene_label}| adding: {row.gene_id} #dx {an}:{row.dbxref} #c {c_text}: {check_var}{extra}\"\n",
    "                )\n",
    "                temp_gene_dict[row.gene_id] = gdt.DbxrefGeneID(\n",
    "                    label=gene_label,\n",
    "                    an_source=an,\n",
    "                    GeneID=row.dbxref,\n",
    "                    c=f\"{c_text}: {check_var}{extra}\",\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                temp_count += 1\n",
    "                label = f\"{gct}-TEMP-{temp_count}\"\n",
    "                log.trace(\n",
    "                    f\"\\t[2nd F]check_var not found in {temp_name}, new label |{label}| adding: {row.gene_id} #dx {an}:{row.dbxref} #c {c_text}: {check_var}{extra}\"\n",
    "                )\n",
    "                temp_gene_dict[check_var] = gdt.GeneDescription(\n",
    "                    label=label,\n",
    "                    source=gn_tag,\n",
    "                    c=None,\n",
    "                )\n",
    "\n",
    "                temp_gene_dict[row.gene_id] = gdt.DbxrefGeneID(\n",
    "                    label=label,\n",
    "                    an_source=an,\n",
    "                    GeneID=row.dbxref,\n",
    "                    c=f\"{c_text}: {check_var}{extra}\",\n",
    "                )\n",
    "\n",
    "    return gene_dict, temp_gene_dict, temp_count\n",
    "\n",
    "\n",
    "def fmt_query(query_raw):\n",
    "    \"\"\"Format the search results from Entrez into a DataFrame.\"\"\"\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"dbxref\": x.attributes[\"uid\"],\n",
    "                \"other_aliases\": x.get(\"OtherAliases\", \"no_other_aliases\"),\n",
    "                \"desc\": x.get(\"Description\", \"no_description\"),\n",
    "                \"gene_symbol\": x.get(\"Name\", \"no_gene_symbol\"),\n",
    "            }\n",
    "            for x in query_raw\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def _producer_worker_raw(\n",
    "    query_string: str,\n",
    "    remove_orfs: bool,\n",
    "    batch: list[tuple[str, Path]],\n",
    ") -> tuple[list[tuple[str, dict[str, list]]], list[str] | None]:\n",
    "    an_df = []\n",
    "    all_dbxrefs = set()\n",
    "    errors = []\n",
    "    an_to_remove = []\n",
    "\n",
    "    for an, gff_path in batch:\n",
    "        df = gdt.load_gff3(gff_path, query_string=query_string)\n",
    "        df = gdt.filter_orfs(df) if remove_orfs else df\n",
    "\n",
    "        df[\"gene_id\"] = df[\"attributes\"].str.extract(RE_ID, expand=False)\n",
    "        df[\"gene\"] = df[\"attributes\"].str.extract(RE_gene, expand=False)\n",
    "        df[\"has_gene\"] = df[\"gene\"].notna()\n",
    "        df[\"dbxref\"] = df[\"attributes\"].str.extract(RE_dbxref_GeneID, expand=False)\n",
    "\n",
    "        all_dbxrefs.update(df[\"dbxref\"])\n",
    "        an_df.append((an, df))\n",
    "\n",
    "    # ncbi query\n",
    "    big_query = \",\".join(all_dbxrefs)\n",
    "    try:\n",
    "        with Entrez.esummary(db=\"gene\", id=big_query) as handle:\n",
    "            query_raw = Entrez.read(handle)[\"DocumentSummarySet\"][\"DocumentSummary\"]  # type: ignore\n",
    "\n",
    "        result = fmt_query(query_raw)\n",
    "\n",
    "    except (RuntimeError, KeyError, Exception):\n",
    "        # retrying query, but this time an per an\n",
    "        result = pd.DataFrame()\n",
    "        errors.append(f\"Batched query failed, query: '{big_query}'\")\n",
    "\n",
    "        for an, df in an_df:\n",
    "            query = \",\".join(df[\"dbxref\"].unique())\n",
    "            with Entrez.esummary(db=\"gene\", id=query) as handle:\n",
    "                try:\n",
    "                    query_raw = Entrez.read(handle)[\"DocumentSummarySet\"][\"DocumentSummary\"]  # type: ignore\n",
    "\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"Error '{e}' in querying {an} with dbxrefs {query}\")\n",
    "                    an_to_remove.append(an)\n",
    "                    continue\n",
    "\n",
    "                result = pd.concat([result, fmt_query(query_raw)])\n",
    "\n",
    "    result_merged = []\n",
    "    for an, df in an_df:\n",
    "        if an in an_to_remove:\n",
    "            continue\n",
    "\n",
    "        df_merged = df.merge(result, on=\"dbxref\", how=\"left\", copy=False)\n",
    "        result_merged.append((an, df_merged.to_dict(\"list\")))\n",
    "\n",
    "    return (\n",
    "        result_merged,\n",
    "        errors,\n",
    "    )\n",
    "\n",
    "\n",
    "def batched(an_col, pb, n):\n",
    "    \"\"\"Batch data into tuples of tuples of length n. The last batch may be shorter.\n",
    "\n",
    "    Args:\n",
    "        an_col: A pandas DataFrame column (e.g., tsv['AN'])\n",
    "        pb: PathBuilder Object with a build() method that takes a AN, returning a path.\n",
    "        n: Batch size, must be at least 1.\n",
    "\n",
    "    Yields:\n",
    "        Tuples of tuples in format ((an, path_builder.build(an)), ...)\n",
    "\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "\n",
    "    it = iter(an_col)\n",
    "    while ans := tuple(islice(it, n)):\n",
    "        yield tuple((an, pb.build(an)) for an in ans)\n",
    "\n",
    "\n",
    "def check_envs(env_path):\n",
    "    \"\"\"Check and load environment variables for NCBI Entrez email and API key.\n",
    "\n",
    "    If the 'dotenv' package is installed, it will load the environment variables\n",
    "    from the specified .env file. If not, it will only check the environment variables.\n",
    "    If the email is not set, it raises a ValueError with instructions on how to set it.\n",
    "    If the API key is not set, it will print a warning about the default request limits.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import dotenv\n",
    "\n",
    "        dotenv.load_dotenv(env_path)\n",
    "\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"Since the 'dotenv' package is not installed, check_envs will only check environment variables.\"\n",
    "        )\n",
    "\n",
    "    email = os.getenv(\"ENTREZ_EMAIL\")\n",
    "    api_key = os.getenv(\"ENTREZ_API_KEY\")\n",
    "\n",
    "    if email:\n",
    "        print(f\"Loading email: {email}\")\n",
    "        Entrez.email = os.getenv(\"ENTREZ_EMAIL\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Entrez email is not set in the environment variables either,\\n\"\n",
    "            \"so please set it in the Setup cell, or set the environment variable 'ENTREZ_EMAIL' directly,\\n\"\n",
    "            \"or create an .env file with it, and pass its path to check_envs.\"\n",
    "        )\n",
    "\n",
    "    if api_key:\n",
    "        print(\"NCBI API Key found, loading it.\")\n",
    "        Entrez.api_key = api_key\n",
    "    elif not Entrez.api_key:\n",
    "        print(\n",
    "            \"Entrez API key is defined, neither set up in the environment variables.\\n\"\n",
    "            \"This will limit the number of requests per second to NCBI to 3, as per\\n\"\n",
    "            \"Biopython Entrez guidelines: https://biopython.org/docs/1.76/api/Bio.Entrez.html \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbc353",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a03048",
   "metadata": {},
   "source": [
    "#### A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines all the global variables used in the script.\n",
    "# Change these variables to match your local setup.\n",
    "# The most_recent_gdt_file variable should be set to the path of the most recent GDT file,\n",
    "# OR the stripped GDT file used in filter command, if applicable.\n",
    "# gct = genetic_compartment\n",
    "\n",
    "DATA_DIR = \"/home/brenodupin/matheus/tigre/sandbox/viruses_seg\"\n",
    "# newest_gdict_file = \"plants_plt_pilot_02.gdict\"\n",
    "global_query_string = gdt.QS_GENE_TRNA_RRNA\n",
    "remove_orfs = True\n",
    "in_folder = True\n",
    "gct = \"SEG\"\n",
    "gff_ext = \".gff3\"\n",
    "gff_suffix = \"\"\n",
    "workers = os.cpu_count() - 1 or 1\n",
    "\n",
    "Entrez.email = \"\"\n",
    "Entrez.api_key = \"\"\n",
    "\n",
    "print(f\"Chosen feature query string: '{global_query_string}'\\n\")\n",
    "\n",
    "# just checking\n",
    "DATA_DIR = Path(DATA_DIR).resolve()\n",
    "if not DATA_DIR.is_dir():\n",
    "    raise FileNotFoundError(f\"Path {DATA_DIR} is not a directory.\")\n",
    "\n",
    "MISC_DIR = DATA_DIR / \"misc\"\n",
    "GDT_DIR = MISC_DIR / \"gdt\"\n",
    "GDT_DIR.mkdir(511, True, True)  # 511 = 0o777\n",
    "\n",
    "AN_missing_gene_dict = MISC_DIR / \"AN_missing_gene_dict.txt\"\n",
    "\n",
    "if not AN_missing_gene_dict.is_file():\n",
    "    raise FileNotFoundError(f\"Missing {AN_missing_gene_dict}, did you run gdt-cli filter?\")\n",
    "    pass\n",
    "\n",
    "if \"newest_gdict_file\" in globals():\n",
    "    gdict_path = GDT_DIR / newest_gdict_file\n",
    "    if not gdict_path.is_file():\n",
    "        print(\n",
    "            f\"Not found {gdict_path.name}, does it exist in misc/gdt?\\nGDICTs in {GDT_DIR}:\"\n",
    "        )\n",
    "        [print(f\" - {f.name}\") for f in sorted(GDT_DIR.glob(\"*.gdt\"))]\n",
    "        raise FileNotFoundError(\n",
    "            f\"Most recent GDIDT file {gdict_path.name} does not exist in {GDT_DIR}.\"\n",
    "        )\n",
    "else:\n",
    "    print(\n",
    "        \"Warning: 'newest_gdict_file' variable not set.\\n\\n\"\n",
    "        \"If you have a previous GDICT file:\\n\"\n",
    "        \"• Set the newest_gdict_file variable\\n\"\n",
    "        \"• Re-run this cell\\n\\n\"\n",
    "        \"If you intend to run this without a GDICT file, this warning can be ignored.\"\n",
    "    )\n",
    "    # to simplify the code, a exetution without newest_gdict_file\n",
    "    # basically the same as with one, but with and empty gdt file\n",
    "    gdict_path = GDT_DIR / \"pilot_00.gdict\"\n",
    "    gdt.create_empty_gdict(gdict_path, overwrite=True)\n",
    "\n",
    "if in_folder:\n",
    "    gff_builder = gdt.PathBuilder(gff_ext).use_folder_builder(DATA_DIR, gff_suffix)\n",
    "else:\n",
    "    gff_builder = gdt.PathBuilder(gff_ext).use_standard_builder(DATA_DIR, gff_suffix)\n",
    "print(f\"PathBuilder: {gff_builder}\\n\")\n",
    "\n",
    "if not Entrez.email:\n",
    "    print(\"Entrez email not set, checking if it's in a .env or in the environment.\")\n",
    "    check_envs(\".env\")\n",
    "\n",
    "print(\"\\nEverything is set up, no problems found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132dad79",
   "metadata": {},
   "source": [
    "#### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61084bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = MISC_DIR / \"01_missing_gene_dict.log\"\n",
    "\n",
    "log = gdt.create_logger(\n",
    "    print_to_console=True,\n",
    "    console_level=\"INFO\",\n",
    "    save_to_file=True,\n",
    "    file_level=\"TRACE\",\n",
    "    log_file=log_file,\n",
    ")\n",
    "log.info(\"Running from notebook AN_missing_gene_dict.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f30404",
   "metadata": {},
   "source": [
    "### TEMP First Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a9ed0",
   "metadata": {},
   "source": [
    "#### A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64970dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(AN_missing_gene_dict, \"r\") as f:\n",
    "    ANs = [line.strip() for line in f.readlines() if line.strip()]\n",
    "log.info(f\"Found {len(ANs)} ANs in {AN_missing_gene_dict}\")\n",
    "log.trace(f\"ANs: {ANs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bd601",
   "metadata": {},
   "source": [
    "#### B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ad533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GDICT file (even if empty)\n",
    "gene_dict = gdt.read_gdict(gdict_path, lazy_info=False)\n",
    "log.info(f\"GeneDict loaded from {gdict_path.name}\")\n",
    "log.debug(f\"path: {gdict_path}\")\n",
    "\n",
    "log.info(\"Header:\")\n",
    "for x in gene_dict.header:\n",
    "    log.info(f\"\\t{x}\")\n",
    "\n",
    "log.info(\"GDT Info:\")\n",
    "gdt.log_info(log, gene_dict)\n",
    "\n",
    "# set up the temporary gene dictionary\n",
    "temp_gene_dict = gdt.GeneDict()\n",
    "temp_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df6622",
   "metadata": {},
   "source": [
    "#### C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ef04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "producer_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "log.info(\" ---- [Starting 'AN_missing_gene_dict parsing gene= + NCBI Description'] ----\")\n",
    "errors = []\n",
    "batches = batched(ANs, gff_builder, batch_size)\n",
    "total_batches = (len(ANs) + batch_size - 1) // batch_size\n",
    "i = 1\n",
    "log.info(f\"Starting {total_batches} batches, sized {batch_size}\")\n",
    "\n",
    "# check all GFFs files exist before processing\n",
    "# TODO\n",
    "\n",
    "producer_worker = partial(_producer_worker_raw, global_query_string, remove_orfs)\n",
    "\n",
    "with cf.ProcessPoolExecutor(max_workers=producer_workers) as ex:\n",
    "    for data, error in ex.map(producer_worker, batches):\n",
    "        if error:\n",
    "            log.warning(\"Got an error in producer: \")\n",
    "            errors.extend(error)\n",
    "            for msg in error:\n",
    "                log.warning(f\"{msg}\")\n",
    "\n",
    "        log.info(\n",
    "            f\"Processing batch {i:0{len(str(total_batches))}d}/{total_batches} ANs: {[x[0] for x in data]}\"\n",
    "        )\n",
    "\n",
    "        for an, prot_df in data:\n",
    "            log.debug(f\"-- [Processing: {an}] --\")\n",
    "\n",
    "            df_merged = pd.DataFrame(prot_df)\n",
    "            df_merged[\"in_gene_dict\"] = df_merged[\"gene_id\"].isin(gene_dict)\n",
    "            df_merged = df_merged[~df_merged[\"in_gene_dict\"]]\n",
    "\n",
    "            # in case NCBI did not return any results for some dbxrefs, or no gff gene\n",
    "            df_merged[\"other_aliases\"] = df_merged[\"other_aliases\"].fillna(\n",
    "                \"no_other_aliases\"\n",
    "            )\n",
    "            df_merged[\"desc\"] = df_merged[\"desc\"].fillna(\"no_description\")\n",
    "            df_merged[\"gene_symbol\"] = df_merged[\"gene_symbol\"].fillna(\"no_gene_symbol\")\n",
    "            df_merged[\"gene\"] = df_merged[\"gene\"].fillna(\"no_gff_gene\")\n",
    "\n",
    "            # process gene= first\n",
    "            df_gene = df_merged[df_merged[\"has_gene\"]]\n",
    "            if not df_gene.empty:\n",
    "                log.debug(f\"Found {len(df_gene)} feature(s) with `gene=`.\")\n",
    "\n",
    "                gene_dict, temp_gene_dict, temp_count = data_process(\n",
    "                    df_gene,\n",
    "                    an,\n",
    "                    gene_dict,\n",
    "                    temp_gene_dict,\n",
    "                    gct,\n",
    "                    temp_count,\n",
    "                    log,\n",
    "                    use_col=\"gene\",\n",
    "                    temp_name=\"temp_gene\",\n",
    "                    c_text=\"gff_gene\",\n",
    "                    gn_tag=\"gff_gene\",\n",
    "                )\n",
    "\n",
    "            # process of ncbi description second\n",
    "            df_desc = df_merged[~df_merged[\"has_gene\"]]\n",
    "            if not df_desc.empty:\n",
    "                log.debug(f\"Found {len(df_desc)} feature(s) with no `gene=`.\")\n",
    "\n",
    "                gene_dict, temp_gene_dict, temp_count = data_process(\n",
    "                    df_desc,\n",
    "                    an,\n",
    "                    gene_dict,\n",
    "                    temp_gene_dict,\n",
    "                    gct,\n",
    "                    temp_count,\n",
    "                    log,\n",
    "                )\n",
    "        i += 1\n",
    "\n",
    "log.info(\"gene_dict and temp_gene_dict before parsing comments:\")\n",
    "gene_dict.update_info()\n",
    "temp_gene_dict.update_info()\n",
    "log.info(\"gene_dict:\")\n",
    "gdt.log_info(log, gene_dict)\n",
    "log.info(\"temp_gene_dict:\")\n",
    "gdt.log_info(log, temp_gene_dict)\n",
    "\n",
    "gene_dict, temp_gene_dict = gdt.parse_via_comments(\n",
    "    gene_dict, temp_gene_dict, lazy_info=False\n",
    ")\n",
    "\n",
    "log.info(\"gene_dict and temp_gene_dict after parsing comments:\")\n",
    "log.info(\"gene_dict:\")\n",
    "gdt.log_info(log, gene_dict)\n",
    "log.info(\"temp_gene_dict:\")\n",
    "gdt.log_info(log, temp_gene_dict)\n",
    "log.info(\" ---- [Finished 'AN_missing_gene_dict parsing gene= + NCBI Description'] ----\")\n",
    "if errors:\n",
    "    log.warning(f\"Errors: {len(errors)}\")\n",
    "    for msg in errors:\n",
    "        log.warning(f\"{msg}\")\n",
    "\n",
    "    log.warning(\n",
    "        \"Entrez.read errors: This is usually a sporadic event or invalid database references.\"\n",
    "    )\n",
    "    log.warning(\"Next steps to diagnose:\")\n",
    "    log.warning(\n",
    "        \"1. Manually verify a few dbxrefs from your GFF file by searching them in NCBI\"\n",
    "    )\n",
    "    log.warning(\"2. If the dbxrefs are valid in NCBI:\")\n",
    "    log.warning(\"   - Save the current gene_dict as your latest GDICT file,\")\n",
    "    log.warning(\"     and update it at the Setup.\")\n",
    "    log.warning(\"   - Re-run this section (the issue was likely temporary)\")\n",
    "    log.warning(\"3. If the dbxrefs are invalid/obsolete:\")\n",
    "    log.warning(\"   - Option A: Remove said ANs from your dataset\")\n",
    "    log.warning(\"   - Option B: Manually remove ANs from AN_missing_gene_dict.txt)\")\n",
    "    log.warning(\"     and add those ANs to AN_missing_dbxref.txt instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e750afd1",
   "metadata": {},
   "source": [
    "#### D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if temp_gene_dict:\n",
    "    temp_path = most_recent_gdict(GDT_DIR, prefix=\"TEMP_Description\")\n",
    "    new_path, nth_iteration = increment_gdict_file(temp_path)\n",
    "    log.info(f\"Writing {new_path.name} to {new_path} | Iteration: {nth_iteration}\")\n",
    "    temp_gene_dict.header = [\n",
    "        \"version 0.0.2\",\n",
    "        f\"TEMP_Description - {nth_iteration}\",\n",
    "        f\"{gdt.time_now()} - Automatically generated from 'AN_missing_gene_dict parsing gene= + NCBI Description'\",\n",
    "    ]\n",
    "    temp_gene_dict.to_gdict(new_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd18eb",
   "metadata": {},
   "source": [
    "#### E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363af008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving gene_dict with the new data, dont forget to change the newest_gdict_file variable\n",
    "new_path, nth_iteration = increment_gdict_file(gdict_path)\n",
    "log.info(f\"Writing {new_path.name} to {new_path} | Iteration: {nth_iteration}\")\n",
    "gene_dict.header.append(\n",
    "    f\"{gdt.time_now()} - Data added from 'AN_missing_gene_dict parsing gene= + NCBI Description'\"\n",
    ")\n",
    "gene_dict.to_gdict(new_path, overwrite=True)\n",
    "log.info(f\"{new_path.name} was created in misc/gdt!\")\n",
    "log.info(\"You must now add it to newest_gdict_file in the Setup cell, and rerun the cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728db9b1",
   "metadata": {},
   "source": [
    "### TEMP Second Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcdbfd",
   "metadata": {},
   "source": [
    "#### A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS_Symbol = set()\n",
    "\n",
    "with open(MISC_DIR / \"seed_TEMP_Symbol.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line or line.startswith(\"#\") or line.startswith(\"[\") or \"#gd\" in line:\n",
    "            continue  # skip these lines\n",
    "\n",
    "        try:\n",
    "            ANS_Symbol.add(line.split(\"#dx\", 1)[1].strip().split(\":\", 1)[0])\n",
    "        except IndexError:\n",
    "            log.warning(f\"Check this line: {line}\")\n",
    "            continue\n",
    "log.debug(\"If you had any warnings, check them and rerun this step.\")\n",
    "log.info(f\"Found {len(ANS_Symbol)} ANs in seed_TEMP_Symbol.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6dd0e",
   "metadata": {},
   "source": [
    "#### B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4da521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GDICT file (even if empty),\n",
    "# if you are running this right after the previous step,\n",
    "# dont forget to change the newest_gdict_file variable\n",
    "# and re-run the setup\n",
    "\n",
    "gene_dict = gdt.read_gdict(gdict_path, lazy_info=False)\n",
    "log.info(f\"GeneDict loaded from {gdict_path.name}\")\n",
    "log.debug(f\"path: {gdict_path}\")\n",
    "\n",
    "log.info(\"Header:\")\n",
    "for x in gene_dict.header:\n",
    "    log.info(f\"\\t{x}\")\n",
    "\n",
    "log.info(\"GDT Info:\")\n",
    "gdt.log_info(log, gene_dict)\n",
    "\n",
    "temp_symbol_gene_dict = gdt.GeneDict()\n",
    "temp_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2810cd1",
   "metadata": {},
   "source": [
    "#### C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55886959",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "log.info(\" ---- [Starting 'AN_missing_gene_dict parsing NCBI Gene Symbol'] ----\")\n",
    "for i, AN in enumerate(ANS_Symbol):\n",
    "    log.info(f\"-- [Processing: {AN}] --\")\n",
    "\n",
    "    an_path = gff_builder.build(AN)\n",
    "    if not an_path.exists():\n",
    "        log.error(f\"Error: {AN} does not exist (an_path: {an_path})\")\n",
    "        errors.append((AN, \"File not found\"))\n",
    "        continue\n",
    "\n",
    "    df = gdt.load_gff3(an_path, query_string=global_query_string)\n",
    "    df = gdt.filter_orfs(df) if remove_orfs else df\n",
    "\n",
    "    # getting the gene_id and if it is in the gene_dict\n",
    "    df[\"gene_id\"] = df[\"attributes\"].str.extract(RE_ID, expand=False)\n",
    "    df[\"in_gene_dict\"] = df[\"gene_id\"].isin(gene_dict)\n",
    "    df[\"gene\"] = df[\"attributes\"].str.extract(RE_gene, expand=False)\n",
    "    df[\"has_gene\"] = df[\"gene\"].notna()\n",
    "    df[\"dbxref\"] = df[\"attributes\"].str.extract(RE_dbxref_GeneID, expand=False)\n",
    "\n",
    "    df_missing = df[~df[\"in_gene_dict\"]].copy()\n",
    "    if df_missing.empty:\n",
    "        log.debug(\"All features are in gene_dict. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # search NCBI\n",
    "    with Entrez.esummary(\n",
    "        db=\"gene\", id=\",\".join(df_missing[\"dbxref\"].unique())\n",
    "    ) as search_handle:\n",
    "        try:\n",
    "            search_results = Entrez.read(search_handle)[\"DocumentSummarySet\"][\"DocumentSummary\"]  # type: ignore\n",
    "        except (RuntimeError, KeyError, Exception) as ex:\n",
    "            log.error(f\"{ex} in Entrez.read for {AN}\")\n",
    "            errors.append((AN, \"Entrez.read\"))\n",
    "            continue\n",
    "\n",
    "    if len(search_results) != len(df_missing[\"dbxref\"].unique()):\n",
    "        log.warning(\n",
    "            f\"Number of search results ({len(search_results)}) does not match number of dbxrefs ({len(df_missing['dbxref'].unique())}) for {AN}.\"\n",
    "        )\n",
    "        missing_dbxrefs = set(df_missing[\"dbxref\"].unique()) - set(\n",
    "            x.attributes[\"uid\"] for x in search_results\n",
    "        )\n",
    "        log.warning(f\"Missing dbxrefs: {missing_dbxrefs}\")\n",
    "        log.warning(\"The missing dbxrefs will be under the 'no_description' tag.\")\n",
    "\n",
    "    # format the search results into a DataFrame\n",
    "    temp_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"dbxref\": x.attributes[\"uid\"],\n",
    "                \"other_aliases\": x.get(\"OtherAliases\", \"no_other_aliases\"),\n",
    "                \"desc\": x.get(\"Description\", \"no_description\"),\n",
    "                \"gene_symbol\": x.get(\"Name\", \"no_gene_symbol\"),\n",
    "            }\n",
    "            for x in search_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_merged = df_missing.merge(temp_df, on=\"dbxref\", how=\"left\", copy=False)\n",
    "\n",
    "    # in case NCBI did not return any results for some dbxrefs\n",
    "    df_merged[\"other_aliases\"] = df_merged[\"other_aliases\"].fillna(\"no_other_aliases\")\n",
    "    df_merged[\"desc\"] = df_merged[\"desc\"].fillna(\"no_description\")\n",
    "    df_merged[\"gene_symbol\"] = df_merged[\"gene_symbol\"].fillna(\"no_gene_symbol\")\n",
    "    df_merged[\"gene\"] = df_merged[\"gene\"].fillna(\"no_gff_gene\")\n",
    "\n",
    "    # process of ncbi gene symbol\n",
    "    gene_dict, temp_symbol_gene_dict, temp_count = data_process(\n",
    "        df_merged,\n",
    "        AN,\n",
    "        gene_dict,\n",
    "        temp_symbol_gene_dict,\n",
    "        gct,\n",
    "        temp_count,\n",
    "        log,\n",
    "        use_col=\"gene_symbol\",\n",
    "        temp_name=\"temp_symbol\",\n",
    "        c_text=\"ncbi_symbol\",\n",
    "    )\n",
    "\n",
    "\n",
    "log.info(\" ---- [Finished 'AN_missing_gene_dict parsing NCBI Gene Symbol'] ----\")\n",
    "if errors:\n",
    "    log.warning(f\"Errors: {len(errors)}\")\n",
    "    for an, msg in errors:\n",
    "        log.warning(f\"  {an} - {msg}\")\n",
    "    log.warning(\n",
    "        \"Entrez.read errors: This is usually a sporadic event or invalid database references.\"\n",
    "    )\n",
    "    log.warning(\"Next steps to diagnose:\")\n",
    "    log.warning(\n",
    "        \"1. Manually verify a few dbxrefs from your GFF file by searching them in NCBI\"\n",
    "    )\n",
    "    log.warning(\"2. If the dbxrefs are valid in NCBI:\")\n",
    "    log.warning(\"   - Save the current gene_dict as your latest GDT file\")\n",
    "    log.warning(\"   - Re-run this section (the issue was likely temporary)\")\n",
    "    log.warning(\"3. If the dbxrefs are invalid/obsolete:\")\n",
    "    log.warning(\"   - Option A: Remove said ANs from your dataset\")\n",
    "    log.warning(\"   - Option B: Manually remove ANs from AN_missing_gene_dict.txt)\")\n",
    "    log.warning(\"     and add those ANs to AN_missing_dbxref.txt instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828bbca",
   "metadata": {},
   "source": [
    "#### D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if temp_symbol_gene_dict:\n",
    "    temp_path = most_recent_gdict(GDT_DIR, prefix=\"TEMP_Symbol\")\n",
    "    new_path, symbol_iteration = increment_gdict_file(temp_path)\n",
    "    log.info(f\"Writing {new_path.name} to {new_path} | Iteration: {symbol_iteration}\")\n",
    "    temp_symbol_gene_dict.header = [\n",
    "        \"version 0.0.2\",\n",
    "        f\"TEMP Symbol - {symbol_iteration}\",\n",
    "        f\"{gdt.time_now()} - Automatically generated from 'AN_missing_gene_dict parsing NCBI Gene Symbol'\",\n",
    "    ]\n",
    "    temp_symbol_gene_dict.to_gdict(new_path, overwrite=True)\n",
    "else:\n",
    "    log.info(\"No TEMP Symbol GDT file created, meaning no unknown gene symbols were found.\")\n",
    "    symbol_iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb95e7f",
   "metadata": {},
   "source": [
    "#### E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving gene_dict with the new data, dont forget to change the newest_gdict_file variable\n",
    "new_path, nth_iteration = increment_gdict_file(gdict_path)\n",
    "log.info(f\"Writing {new_path.name} to {new_path} | pilot itr: {nth_iteration}\")\n",
    "gene_dict.header.append(\n",
    "    f\"{gdt.time_now()} - Data added from 'AN_missing_gene_dict parsing NCBI Gene Symbol'\"\n",
    ")\n",
    "gene_dict.to_gdict(new_path, overwrite=True)\n",
    "log.info(f\"{new_path.name} was created in misc/gdt!\")\n",
    "log.info(\"You must now add it to newest_gdict_file in the Setup cell, and rerun the cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69fe05",
   "metadata": {},
   "source": [
    "### Genes exclusion of to_exclude.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a356f",
   "metadata": {},
   "source": [
    "#### A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_string = \"discard-\"\n",
    "genes_to_exclude = \"to_exclude.txt\"\n",
    "\n",
    "exclude_gene_ids = defaultdict(set)\n",
    "with open(MISC_DIR / genes_to_exclude, \"r\") as f:\n",
    "    for line in f:\n",
    "        if (\n",
    "            not line.strip()\n",
    "            or line.startswith(\"#\")\n",
    "            or line.startswith(\"[\")\n",
    "            or \"#gd\" in line\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            gene_id, an = line.split(\"#c\", 1)[0].split(\"#dx\", 1)\n",
    "            an = an.split(\":\", 1)[0].strip()\n",
    "            exclude_gene_ids[an].add(gene_id.strip())\n",
    "        except ValueError:\n",
    "            log.warning(f\"Check this line: {line.strip()}\")\n",
    "            continue\n",
    "\n",
    "log.info(f\"Found {len(exclude_gene_ids)} ANs in {genes_to_exclude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fcb5aa",
   "metadata": {},
   "source": [
    "#### B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17773f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\" ---- [Starting 'AN_missing_gene_dict excluding gene IDs from GFF3s'] ----\")\n",
    "for an in exclude_gene_ids.keys():\n",
    "    log.info(f\"Processing {an} for excluding {len(exclude_gene_ids[an])} gene IDs\")\n",
    "    log.trace(f\" excluding gene IDs: {exclude_gene_ids[an]}\")\n",
    "    an_path = gff_builder.build(an)\n",
    "    with open(an_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    headers, index = [], 0\n",
    "    while lines[index].startswith(\"#\"):\n",
    "        headers.append(lines[index].strip())\n",
    "        index += 1\n",
    "\n",
    "    ids = [re.escape(x) for x in exclude_gene_ids[an]]\n",
    "    pattern = re.compile(rf\"(?:ID|Parent)=(?:{'|'.join(ids)})(?:;|$)\")\n",
    "    log.trace(f\"Pattern for exclusion: {pattern.pattern}\")\n",
    "    contents = []\n",
    "\n",
    "    for line in lines[index:]:\n",
    "        if not (line := line.strip()):\n",
    "            continue\n",
    "        line = line.split(\"\\t\")\n",
    "\n",
    "        # line[2] is type, line[8] is attributes\n",
    "        if pattern.search(line[8]):\n",
    "            if append_string not in line[2]:\n",
    "                line[2] = append_string + line[2]\n",
    "\n",
    "        contents.append(\"\\t\".join(line))\n",
    "\n",
    "    with open(an_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(headers))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\".join(contents))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "log.info(\" ---- [Finished 'AN_missing_gene_dict excluding gene IDs from GFF3s'] ----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
